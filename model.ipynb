{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":21733,"databundleVersionId":1408234,"sourceType":"competition"}],"dockerImageVersionId":30527,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\nThis starter notebook is provided by the Keras team.</center>\n\n## Keras NLP starter guide here: https://keras.io/guides/keras_nlp/getting_started/\n\n__This starter notebook uses the [BERT](https://arxiv.org/abs/1810.04805) pretrained model from KerasNLP.__\n\n**BERT** stands for **Bidirectional Encoder Representations from Transformers**. BERT and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models.\n\nThe BERT family of models uses the **Transformer encoder architecture** to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers.\n\nBERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks.\n\n\n![BERT Architecture](http://miro.medium.com/v2/resize:fit:1032/0*x3vhaoJdGndvZqmL.png)\n\n\nThis notebook contains complete code to fine-tune BERT to perform a **Natural Language Inferencing (NLI)** model. NLI is a popular NLP problem that involves determining how pairs of sentences (consisting of a premise and a hypothesis) are related.\n\nOur **NLI model** will assign labels of 0, 1, or 2 (corresponding to **entailment, neutral, and contradiction**) to pairs of premises and hypotheses.\n\nNote that the train and test set include text in **fifteen different languages**!\n\n\nIn this notebook, you will:\n\n- Load the Contradictory, My Dear Watson dataset\n- Explore the dataset\n- Preprocess the data\n- Load a BERT model from Keras NLP\n- Train your own model, fine-tuning BERT as part of that\n- Generate the submission file\n","metadata":{}},{"cell_type":"code","source":"!pip install -q keras-nlp --upgrade\n!pip install seaborn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-03T12:40:55.470521Z","iopub.execute_input":"2023-08-03T12:40:55.470871Z","iopub.status.idle":"2023-08-03T12:41:05.235148Z","shell.execute_reply.started":"2023-08-03T12:40:55.470843Z","shell.execute_reply":"2023-08-03T12:41:05.233965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow import keras\nimport keras_nlp\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"KerasNLP version:\", keras_nlp.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-08-03T12:41:05.23739Z","iopub.execute_input":"2023-08-03T12:41:05.237747Z","iopub.status.idle":"2023-08-03T12:41:36.37782Z","shell.execute_reply.started":"2023-08-03T12:41:05.237712Z","shell.execute_reply":"2023-08-03T12:41:36.376729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Accelerator\n\nDetect hardware, return appropriate distribution strategy","metadata":{}},{"cell_type":"code","source":" try:\n    # detect and init the TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    # instantiate a distribution strategy\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept ValueError:\n    print(\"TPU not activated\")\n    strategy = tf.distribute.MirroredStrategy() # Works on CPU, single GPU and multiple GPUs in a single VM.\n    \nprint(\"replicas:\", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2023-08-03T12:41:43.632365Z","iopub.execute_input":"2023-08-03T12:41:43.633498Z","iopub.status.idle":"2023-08-03T12:41:52.592748Z","shell.execute_reply.started":"2023-08-03T12:41:43.633459Z","shell.execute_reply":"2023-08-03T12:41:52.591569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the Contradictory, My Dear Watson dataset\nLet's have a look at all the data files\n\nThe training set contains a premise, a hypothesis, a label (0 = entailment, 1 = neutral, 2 = contradiction), and the language of the text. For more information about what these mean and how the data is structured, check out the data page: https://www.kaggle.com/c/contradictory-my-dear-watson/data","metadata":{}},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/contradictory-my-dear-watson/'\n\nRESULT_DICT = {\n    0 : \"entailment\",\n    1 : \"neutral\",\n    2 : \"contradiction\"\n}\n\nfor dirname, _, filenames in os.walk(DATA_DIR):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-08-03T12:41:52.594663Z","iopub.execute_input":"2023-08-03T12:41:52.594994Z","iopub.status.idle":"2023-08-03T12:41:52.603048Z","shell.execute_reply.started":"2023-08-03T12:41:52.594965Z","shell.execute_reply":"2023-08-03T12:41:52.602032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(DATA_DIR + \"train.csv\")\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-03T12:41:52.604173Z","iopub.execute_input":"2023-08-03T12:41:52.6045Z","iopub.status.idle":"2023-08-03T12:41:52.735395Z","shell.execute_reply.started":"2023-08-03T12:41:52.604471Z","shell.execute_reply":"2023-08-03T12:41:52.734261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv(DATA_DIR + \"test.csv\")\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-03T12:41:52.738018Z","iopub.execute_input":"2023-08-03T12:41:52.738507Z","iopub.status.idle":"2023-08-03T12:41:52.794524Z","shell.execute_reply.started":"2023-08-03T12:41:52.738472Z","shell.execute_reply":"2023-08-03T12:41:52.793508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at some pairs of sentences.","metadata":{}},{"cell_type":"code","source":"def display_pair_of_sentence(x):\n    print( \"Premise : \" + x['premise'])\n    print( \"Hypothesis: \" + x['hypothesis'])\n    print( \"Language: \" + x['language'])\n    print( \"Label: \" + str(x['label']))\n    print()\n\ndf_train.head(10).apply(lambda x : display_pair_of_sentence(x), axis=1)\n\ndf_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-03T12:41:52.795829Z","iopub.execute_input":"2023-08-03T12:41:52.796189Z","iopub.status.idle":"2023-08-03T12:41:52.808622Z","shell.execute_reply.started":"2023-08-03T12:41:52.796158Z","shell.execute_reply":"2023-08-03T12:41:52.807665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explore the dataset\n\nLet's look at the distribution of labels in the training set.","metadata":{}},{"cell_type":"code","source":"# Initialize the matplotlib figure\nf, ax = plt.subplots(figsize=(12, 4))\n\n# Plot the total crashes\nsns.set_color_codes(\"pastel\")\nsns.despine()\nax = sns.countplot(data=df_train, \n                   y=\"label\",\n                   order = df_train['label'].value_counts().index)\n\nabs_values = df_train['label'].value_counts(ascending=False)\nrel_values = df_train['label'].value_counts(ascending=False, normalize=True).values * 100\nlbls = [f'{p[0]} ({p[1]:.0f}%)' for p in zip(abs_values, rel_values)]\n\nax.bar_label(container=ax.containers[0], labels=lbls)\n\nax.set_yticklabels([RESULT_DICT[index] for index in abs_values.index])\n\nax.set_title(\"Distribution of labels in the training set\")","metadata":{"execution":{"iopub.status.busy":"2023-08-03T12:41:52.809787Z","iopub.execute_input":"2023-08-03T12:41:52.810327Z","iopub.status.idle":"2023-08-03T12:41:53.132724Z","shell.execute_reply.started":"2023-08-03T12:41:52.810268Z","shell.execute_reply":"2023-08-03T12:41:53.131619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the distribution of languages in the training set.","metadata":{}},{"cell_type":"code","source":"# Initialize the matplotlib figure\nf, ax = plt.subplots(figsize=(10, 10))\n\n# Plot the total crashes\nsns.set_color_codes(\"pastel\")\nsns.despine()\nax = sns.countplot(data=df_train, \n                   y=\"language\",\n                   order = df_train['language'].value_counts().index)\n\nabs_values = df_train['language'].value_counts(ascending=False)\nrel_values = df_train['language'].value_counts(ascending=False, normalize=True).values * 100\nlbls = [f'{p[0]} ({p[1]:.0f}%)' for p in zip(abs_values, rel_values)]\n\nax.bar_label(container=ax.containers[0], labels=lbls)\n\nax.set_title(\"Distribution of languages in the training set\")","metadata":{"execution":{"iopub.status.busy":"2023-08-03T12:41:53.134015Z","iopub.execute_input":"2023-08-03T12:41:53.134389Z","iopub.status.idle":"2023-08-03T12:41:53.605813Z","shell.execute_reply.started":"2023-08-03T12:41:53.134359Z","shell.execute_reply":"2023-08-03T12:41:53.60486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the length of the sentences","metadata":{}},{"cell_type":"code","source":"df_train[\"premise_length\"] = df_train[\"premise\"].apply(lambda x : len(x))\ndf_train[\"hypothesis_length\"] = df_train[\"hypothesis\"].apply(lambda x : len(x))\ndf_train[[\"hypothesis_length\", \"premise_length\"]].describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-03T12:41:53.606974Z","iopub.execute_input":"2023-08-03T12:41:53.607265Z","iopub.status.idle":"2023-08-03T12:41:53.64704Z","shell.execute_reply.started":"2023-08-03T12:41:53.607239Z","shell.execute_reply":"2023-08-03T12:41:53.646139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess the data\n\nText inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT.\n\nThe BertClassifier model can  be configured with a preprocessor layer, in which case it will automatically apply preprocessing to raw inputs during fit(), predict(), and evaluate(). This is done by default when creating the model with from_preset().\n\nBert is only trained in English corpus. That's why people use multilingual Bert or XLM-Roberta for this competition.\n\nHere are some models for multi-language NLP available in Keras NLP:\n- bert_base_multi\n- deberta_v3_base_multi\n- distil_bert_base_multi\n- xlm_roberta_base_multi\n- xlm_roberta_large_multi\n","metadata":{}},{"cell_type":"code","source":"VALIDATION_SPLIT = 0.3\nTRAIN_SIZE = int(df_train.shape[0]*(1-VALIDATION_SPLIT))\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\n","metadata":{"execution":{"iopub.status.busy":"2023-08-03T12:41:53.648078Z","iopub.execute_input":"2023-08-03T12:41:53.648532Z","iopub.status.idle":"2023-08-03T12:41:53.653275Z","shell.execute_reply.started":"2023-08-03T12:41:53.648499Z","shell.execute_reply":"2023-08-03T12:41:53.652347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here's a utility function that splits the example into an `(x, y)` tuple that is suitable for `model.fit()`.\n\nBy default, `keras_nlp.models.BertClassifier` will tokenize and pack together raw strings using a `\"[SEP]\"` token during training.\n\nTherefore, this label splitting is all the data preparation that we need to perform.","metadata":{}},{"cell_type":"code","source":"def split_labels(x, y):\n    return (x[0], x[1]), y\n\n\ntraining_dataset = (\n    tf.data.Dataset.from_tensor_slices(\n        (\n            df_train[['premise','hypothesis']].values,\n            keras.utils.to_categorical(df_train['label'], num_classes=3)\n        )\n    )\n)\n\ntrain_dataset = training_dataset.take(TRAIN_SIZE)\nval_dataset = training_dataset.skip(TRAIN_SIZE)\n\n# Apply the preprocessor to every sample of train, val and test data using `map()`.\n# [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data/AUTOTUNE) and `prefetch()` are options to tune performance, see\n# https://www.tensorflow.org/guide/data_performance for details.\n\ntrain_preprocessed = train_dataset.map(split_labels, tf.data.AUTOTUNE).batch(BATCH_SIZE, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)\nval_preprocessed = val_dataset.map(split_labels, tf.data.AUTOTUNE).batch(BATCH_SIZE, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-08-03T12:41:53.655745Z","iopub.execute_input":"2023-08-03T12:41:53.656142Z","iopub.status.idle":"2023-08-03T12:41:53.763476Z","shell.execute_reply.started":"2023-08-03T12:41:53.656113Z","shell.execute_reply":"2023-08-03T12:41:53.762577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load a BERT model from Keras NLP - Train the model","metadata":{"execution":{"iopub.status.busy":"2023-07-31T05:23:02.390523Z","iopub.execute_input":"2023-07-31T05:23:02.390962Z","iopub.status.idle":"2023-07-31T05:23:02.396392Z","shell.execute_reply.started":"2023-07-31T05:23:02.390928Z","shell.execute_reply":"2023-07-31T05:23:02.394937Z"}}},{"cell_type":"code","source":"# Load a BERT model.\n\nwith strategy.scope():\n    classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_base_multi\", num_classes=3)\n\n    # in distributed training, the recommendation is to scale batch size and learning rate with the numer of workers.\n    classifier.compile(optimizer=keras.optimizers.Adam(1e-5*strategy.num_replicas_in_sync),\n                       loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n                       metrics=['accuracy'])\n    \n    classifier.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-03T12:41:56.437108Z","iopub.execute_input":"2023-08-03T12:41:56.437808Z","iopub.status.idle":"2023-08-03T12:42:32.217655Z","shell.execute_reply.started":"2023-08-03T12:41:56.437768Z","shell.execute_reply":"2023-08-03T12:42:32.216286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train your own model - Fine-tuning BERT","metadata":{}},{"cell_type":"code","source":"EPOCHS=3\nhistory = classifier.fit(train_preprocessed,\n                         epochs=EPOCHS,\n                         validation_data=val_preprocessed\n                        )","metadata":{"execution":{"iopub.status.busy":"2023-08-03T12:42:35.543015Z","iopub.execute_input":"2023-08-03T12:42:35.544099Z","iopub.status.idle":"2023-08-03T12:46:31.013639Z","shell.execute_reply.started":"2023-08-03T12:42:35.544055Z","shell.execute_reply":"2023-08-03T12:46:31.012055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate the submission file\n\nLet's get the test data","metadata":{}},{"cell_type":"code","source":"predictions = classifier.predict((df_test['premise'],df_test['hypothesis']), batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-08-03T12:47:09.619964Z","iopub.execute_input":"2023-08-03T12:47:09.621045Z","iopub.status.idle":"2023-08-03T12:47:45.845082Z","shell.execute_reply.started":"2023-08-03T12:47:09.620998Z","shell.execute_reply":"2023-08-03T12:47:45.843478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we create a csv file with exactly 5195 entries plus a header row.\n\nThe file has 2 columns:\n- id (sorted in any order)\n- prediction (contains your predictions: 0 for entailment, 1 for neutral, 2 for contradiction)","metadata":{}},{"cell_type":"code","source":"submission = df_test.id.copy().to_frame()\nsubmission[\"prediction\"] = np.argmax(predictions, axis=1)\n\nsubmission","metadata":{"execution":{"iopub.status.busy":"2023-08-03T12:47:45.848363Z","iopub.execute_input":"2023-08-03T12:47:45.848742Z","iopub.status.idle":"2023-08-03T12:47:45.865675Z","shell.execute_reply.started":"2023-08-03T12:47:45.848708Z","shell.execute_reply":"2023-08-03T12:47:45.864612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you need details on how to submit from a notebook, review the FAQ on [\"How do I make a submission?\"]\n(https://www.kaggle.com/c/contradictory-my-dear-watson/overview/frequently-asked-questions)\n\nGood luck!\n","metadata":{}}]}